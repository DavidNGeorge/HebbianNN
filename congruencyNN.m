%Sample script to run a simulation of concurrent conditional
%discriminations involving four contexts (A, B, C, D) and four cues (W, X,
%Y, Z) and two outcomes. Two clones of each network are separately trained
%on congruent (AW-1 AX-2 AY-1 AZ-2; BW-2 BX-1 AY-2 BZ-1; CW-1 CX-2 CY-1 
%CZ-2; DW-2 DX-1 DY-2 DZ-1; A = C, B = D, W = Y, X = Z), or incongruent 
%(AW-2 AX-1 AY-1 AZ-2; BW-2 BX-1 AY-2 BZ-1; CW-1 CX-2 CY-1 CZ-2; DW-1 DX-2 
%DY-2 DZ-1) versions of the discrimination.
%This script uses the default network settings defined in createNet and the
%minimum number of hidden units required to learn the incongruent version
%(8). 1000 pairs of cloned network are trained and result averaged over
%them. rmse of the networks' predictions over all training patterns is
%calculated for each epoch of training and plotted.
%Learning rate parameter for the different set of weights are set in this
%script which overrides the default values set in createNet.
%Training data averaged over epochs and over networks are stored in
%dataMean and dataSummary, respectively.
%
% David N. George, Jan 2018
nHidden = 8; %number of hidden units
runs = 1000; %number of (pairs of cloned) networks trained
input = [1 0 0 0 1 0 0 0;... %matrix of input patterns
         1 0 0 0 0 1 0 0;... %each row is a different pattern
         1 0 0 0 0 0 1 0;... %each column corresponds to an input unit
         1 0 0 0 0 0 0 1;...
         0 1 0 0 1 0 0 0;...
         0 1 0 0 0 1 0 0;...
         0 1 0 0 0 0 1 0;...
         0 1 0 0 0 0 0 1;...
         0 0 1 0 1 0 0 0;...
         0 0 1 0 0 1 0 0;...
         0 0 1 0 0 0 1 0;...
         0 0 1 0 0 0 0 1;...
         0 0 0 1 1 0 0 0;...
         0 0 0 1 0 1 0 0;...
         0 0 0 1 0 0 1 0;...
         0 0 0 1 0 0 0 1];
cOut = [1 0;... %matrix of output patterns for the congruent discimination
        0 1;... %each row is a different pattern
        1 0;... %each column corresponds to an input unit
        0 1;...
        0 1;...
        1 0;...
        0 1;...
        1 0;...
        1 0;...
        0 1;...
        1 0;...
        0 1;...
        0 1;...
        1 0;...
        0 1;...
        1 0];
iOut = [0 1;... %matrix of output patterns for the incongruent discimination
        1 0;...
        1 0;...
        0 1;...
        0 1;...
        1 0;...
        0 1;...
        1 0;...
        1 0;...
        0 1;...
        1 0;...
        0 1;...
        1 0;...
        0 1;...
        0 1;...
        1 0];
congParam = createSim(input, cOut); %wraps the congruent input and output patterns up in a structure
incongParam = createSim(input, iOut); %wraps the incongruent input and output patterns up in a structure
netParam = createNet(congParam, nHidden); %creates the network settings
netParam.learnRateIH = 0.1; %changes the default learning rate parameters
netParam.learnRateOH = 0.2; %changes the default learning rate parameters
netParam.learnRateHO = 0.2; %changes the default learning rate parameters
congParam.nEpochs = 100; %sets the number of training epochs to 100 (congruent)
incongParam.nEpochs = 100; %sets the number of training epochs to 100 (incongruent)
dataStore = zeros(2, runs, congParam.nEpochs + 1); %matrix to receive training data
%we must create a couple of separate matrices to allow the use of parallel
%processing, we will put them back together later
di1 = zeros(runs, 1); %initial rmse for each network (congruent)
ds1 = zeros(runs, congParam.nEpochs); %training rmse for each network (congruent)
di2 = zeros(runs, 1); %initial rmse for each network (incongruent)
ds2 = zeros(runs, congParam.nEpochs); %training rmse for each network (incongruent)
parfor i = 1:1:runs %cycle through networks (cloned pairs)
    initState = initNet(netParam); %initializes the weight matrices
    di1(i) = mean(sqrt(mean((testNet(netParam, initState, congParam)' - congParam.outputPatterns).^2, 2))); %calculate initial rmse (congruent)
    di2(i) = mean(sqrt(mean((testNet(netParam, initState, incongParam)' - incongParam.outputPatterns).^2, 2))); %calculate initial rmse (incongruent)
    [~, ds1(i, :)] = trainNet(netParam, initState, congParam); %train the network (congruent)
    [~, ds2(i, :)] = trainNet(netParam, initState, incongParam); %train the network (incongruent)
end
dataStore(1, :, :) = cat(2, di1, ds1); %put together the congruent training data
dataStore(2, :, :) = cat(2, di2, ds2); %put together the incongruent training data
clear di1 ds1 di2 ds2 %clear temporary variables
dataMean = squeeze(mean(dataStore, 2)); %average rmse across networks
dataSummary = squeeze(mean(dataStore, 3)); %average rmse over epochs
plot(dataMean') %plot congruent vs incongruent acquisition
legend('congruent', 'incongruent')
%display some summary data
m = mean(dataSummary, 2);
s = std(dataSummary, 0, 2);
disp(['congruent = ' num2str(m(1)) '(' num2str(s(1)) ')']);
disp(['incongruent = ' num2str(m(2)) '(' num2str(s(2)) ')']);
[~, p, ci, sts] = ttest(dataSummary(1, :), dataSummary(2, :));
disp(['t(' num2str(sts.df) ') = ' num2str(sts.tstat) '; p = ' num2str(p)...
    '; diff = ' num2str(m(1) - m(2)) '; 95%CI[' num2str(ci(1)) ', ' num2str(ci(2)) ']']);
%clear some variables
clear nHidden runs input cOut iOut congParam incongParam netParam ...
    dataStore m s p ci sts